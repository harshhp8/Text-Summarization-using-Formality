{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('H:/College WOrk/papers/stop-words.txt',encoding='utf-16') as fp:\n",
    "    v = fp.read()\n",
    "\n",
    "def removestopwords(line):\n",
    "    #print(\"original line\",line)\n",
    "    querywords = line.split()\n",
    "\n",
    "    resultwords  = [word for word in querywords if word.lower() not in v]\n",
    "    result = ' '.join(resultwords)\n",
    "    #print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "from collections import Counter\n",
    "WORD = re.compile(r\"\\w+\")\n",
    "\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x] ** 2 for x in list(vec1.keys())])\n",
    "    sum2 = sum([vec2[x] ** 2 for x in list(vec2.keys())])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('indian')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import tnt \n",
    "from nltk.corpus import indian \n",
    "\n",
    "#train_data = indian.tagged_sents('gu.pos')\n",
    "train_data = indian.tagged_sents('C:/Users/HM-AM/Documents/Jupyter Notebooks/gu.pos')\n",
    "tnt_pos_tagger = tnt.TnT() \n",
    "tnt_pos_tagger.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = int(len(train_data) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sents = train_data\n",
    "train_sents = train_data[:size]\n",
    "test_sents = train_data[size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sentences, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sentences, backoff=backoff)\n",
    "        \n",
    "    return backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_tagger = nltk.DefaultTagger('NN')\n",
    "Combine_tagger = backoff_tagger(train_sents,\n",
    "[nltk.UnigramTagger, nltk.BigramTagger, nltk.TrigramTagger], backoff = back_tagger)\n",
    "\n",
    "Combine_tagger.evaluate(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_to_string(org_list, seperator=' '):\n",
    "    \"\"\" Convert list to string, by joining all item in list with given separator.\n",
    "        Returns the concatenated string \"\"\"\n",
    "    return seperator.join(org_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_there(s):\n",
    "    return any(i.isdigit() for i in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from math import log\n",
    "def shannon(string):\n",
    "    counts = Counter(string)\n",
    "    #print(counts)\n",
    "    frequencies = ((i / len(string)) for i in counts.values())\n",
    "    return - sum(f * log(f, 2) for f in frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def Average(lst):\n",
    "    return mean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "def Average(lst):\n",
    "    return mean(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge.rouge import rouge_n_sentence_level\n",
    "from rouge import rouge_n_summary_level\n",
    "from rouge import rouge_l_summary_level\n",
    "from rouge import rouge_w_sentence_level\n",
    "from rouge import rouge_w_summary_level\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "def comparerouge1(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_1 = rouge_n_summary_level(tokenize_words, tokenize_words1, 1)\n",
    "    #rouge1.append(rouge_1)\n",
    "    #print('ROUGE-1: %f' % rouge_1)\n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "    #print('ROUGE-2: %f' % rouge_2)\n",
    "    #rouge2.append(rouge_2)\n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    #print('ROUGE-L: %f' % rouge_l)\n",
    "    #rougel.append(rouge_l)\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    #avg=(rouge_1+rouge_2+rouge_l+rouge_w)/4\n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerouge2(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "   \n",
    "    _, _, rouge_2 = rouge_n_summary_level(tokenize_words, tokenize_words1, 2)\n",
    "   \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougel(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    \n",
    "    _, _, rouge_l = rouge_l_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_l    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparerougew(reference,originalfull_str,filename):\n",
    "    a = urlparse(filename)\n",
    "    tokenize_words=[]\n",
    "    tokenize_words1=[]\n",
    "    for tokenize in indic_tokenize.trivial_tokenize(originalfull_str):\n",
    "            tokenize_words.append(tokenize)\n",
    "    sentences=sentence_tokenize.sentence_split(originalfull_str, lang='gu')\n",
    "    with codecs.open(reference+\"a2_\"+os.path.basename(a.path), encoding='utf-8') as f1:\n",
    "        data1 = f1.read()\n",
    "    for tokenize1 in indic_tokenize.trivial_tokenize(data1):\n",
    "            tokenize_words1.append(tokenize1)\n",
    "        # Convert list of strings to string\n",
    "    originalfull_str1 = convert_list_to_string(tokenize_words1)\n",
    "    sentences1=sentence_tokenize.sentence_split(originalfull_str1, lang='gu')\n",
    "    _, _, rouge_w = rouge_w_summary_level(tokenize_words, tokenize_words1)\n",
    "    \n",
    "    tokenize_words.clear()\n",
    "    tokenize_words1.clear()\n",
    "    return rouge_w    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import string\n",
    "import re\n",
    "from bs4 import BeautifulSoup  \n",
    "from nltk.tokenize import word_tokenize\n",
    "from indicnlp.tokenize import sentence_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize\n",
    "\n",
    "#variation 1\n",
    "# keywords=['N-NNP','N_NN','JJ','N_NNV','N_NST','QT_QTC','QT_QTF','QT_QTO','PSP']\n",
    "# keywords1=['V_VAUX','RP_RPD','V_VM','V_VM_VF','VM_VM_VINF','V_VM_VNF','V_VM_VNG','PR_PRC','PR_PRF','PR_PRI','PR_PRP','PR_PRQ',\n",
    "#           'RB']\n",
    "# #variation 2\n",
    "# keywords=['N-NNP','N_NN','N_NNV','N_NST','PSP','JJ','RB','V_VAUX','V_VM','V_VM_VF',\n",
    "#           'VM_VM_VINF','V_VM_VNF','V_VM_VNG']\n",
    "# keywords1=['RP_RPD','PR_PRC','PR_PRF','PR_PRI','PR_PRP','PR_PRQ','QT_QTC','QT_QTF','QT_QTO']\n",
    "#variation 3\n",
    "# keywords=['N-NNP','N_NN','N_NNV','N_NST','PSP','JJ','RB','NN','QT_QTC','QT_QTF','QT_QTO']\n",
    "# keywords1=['RP_RPD','PR_PRC','PR_PRF','PR_PRI','PR_PRP','PR_PRQ','V_VAUX','V_VM','V_VM_VF',\n",
    "#           'VM_VM_VINF','V_VM_VNF','V_VM_VNG']\n",
    "# # # variation 4\n",
    "# keywords=['NN','QT_QTC','QT_QTF','QT_QTO','RP_RPD','RP_INTF','JJ','RB',\n",
    "#          'PR_PRC','PR_PRF','PR_PRI','PR_PRP','PR_PRQ','N-NNP','N_NN','N_NNV','N_NST','PSP']\n",
    "# keywords1=['V_VAUX','V_VM','V_VM_VF','VM_VM_VINF','V_VM_VNF','V_VM_VNG','DM_DMD','RD_PUNC','CC_CCD','CC_CCS']\n",
    "# #variation 5\n",
    "# keywords=['N-NNP','N_NN','JJ','N_NNV','N_NST','QT_QTC','QT_QTF','QT_QTO','PSP','RB','CC_CCD','CC_CCS','DM_DMD','NN']\n",
    "# keywords1=['V_VAUX','RP_RPD','V_VM','V_VM_VF','VM_VM_VINF','V_VM_VNF','V_VM_VNG','PR_PRC','PR_PRF','PR_PRI','PR_PRP','PR_PRQ']\n",
    "#variation 6\n",
    "keywords=['N-NNP','N_NN','N_NNV','N_NST','PSP','JJ','NN']\n",
    "keywords1=['RP_RPD','RP_INJ','RP_INTF','PR_PRC','PR_PRF','PR_PRI','PR_PRP','PR_PRQ','RB','V_VAUX','V_VM','V_VM_VF',\n",
    "          'VM_VM_VINF','V_VM_VNF','V_VM_VNG','DM_DMD','DM_DMR','DM_DMQ','CC_CCD','CC_CCS']\n",
    "fc=[]\n",
    "li=[]\n",
    "maintext=[]\n",
    "rouge1=[]\n",
    "rouge2=[]\n",
    "rougel=[]\n",
    "rougew=[]\n",
    "class ShortText:\n",
    "    def __init__(self, my_id, human_label, summary, short_text):\n",
    "        self.id = my_id         \n",
    "        self.human_label = human_label    \n",
    "        self.summary = summary \n",
    "        self.short_text = short_text\n",
    "    def __str__(self):\n",
    "        '''\n",
    "        For printing purposes.\n",
    "        '''\n",
    "        return '%d\\t%d\\t%s\\t%s' % (self.id, self.human_label, self.summary, self.short_text)\n",
    "\n",
    "def load_file8(filename,createfile):\n",
    "    tokenize_words=[]\n",
    "    titlesentences=[]\n",
    "    file1 = open(createfile, \"w\", encoding=\"utf-8\")\n",
    "    #retrieve the original text \n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "    soup = BeautifulSoup(data)\n",
    "    docno=soup.find_all('docno')\n",
    "    text=soup.find_all('text')\n",
    "    titleno = soup.find_all('title')\n",
    "    instances = {}\n",
    "    my_id = 0\n",
    "    for n,tit,maintitle in zip(docno,text,titleno):\n",
    "        count=0\n",
    "        tit=tit.get_text()\n",
    "        tit=str(tit)\n",
    "        #print(\"this is text\",tit)\n",
    "        maintitle=maintitle.get_text()\n",
    "        maintitle=str(maintitle)\n",
    "        maintitlelist=[]\n",
    "        for tokenize in indic_tokenize.trivial_tokenize(maintitle):\n",
    "            titlesentences.append(tokenize)\n",
    "        list8 = [''.join(c for c in s if c not in string.punctuation) for s in titlesentences]\n",
    "        list8 = [s for s in list8 if s]\n",
    "        originalsen=[]\n",
    "        sentences=sentence_tokenize.sentence_split(tit, lang='gu')\n",
    "        #print(sentences)\n",
    "        list3 = [''.join(c for c in s if c not in string.punctuation) for s in sentences]\n",
    "        list3 = [s for s in list3 if s]\n",
    "        #list3 = [s for s in sentences if s]\n",
    "        lensen=0\n",
    "        lengthofsen=0\n",
    "        senlength=8\n",
    "        for t in list3:\n",
    "            sen=removestopwords(t)\n",
    "            #sen=t\n",
    "            #matching with title\n",
    "            if(len(sen)==0):\n",
    "                sen=sen+\"hi\"\n",
    "            querywords = sen.split()\n",
    "            h = len(querywords)\n",
    "            while(h>=0):\n",
    "                if(h==len(querywords)):\n",
    "                    h=h-1\n",
    "                    \n",
    "                for x in range(len(list8)):\n",
    "#                     print(list3[x])\n",
    "#                     print(querywords[j])\n",
    "                    vector1 = text_to_vector(list8[x])\n",
    "                    vector2 = text_to_vector(querywords[h])\n",
    "                    val = get_cosine(vector1, vector2)\n",
    "                    if(val>=1):\n",
    "                        #print(list8[x]+\"matching\"+querywords[h])\n",
    "                        count=count+1\n",
    "                        #print(\"this is count\",count)\n",
    "                #i=i+1\n",
    "                h=h-1\n",
    "            tagged_words = (Combine_tagger.tag(nltk.word_tokenize(sen)))\n",
    "            #originalsen.append(sen)\n",
    "        \n",
    "            lst1=[]\n",
    "            for w in tagged_words:\n",
    "                lst1.append(w[1])\n",
    "\n",
    "\n",
    "            b = nltk.FreqDist()\n",
    "            i=1\n",
    "#             #print(\"this is count\",count)\n",
    "#             i=i+count\n",
    "            j=1\n",
    "            #j=j-count\n",
    "            for w in lst1:\n",
    "                if w in keywords:\n",
    "                    i=i+10\n",
    "\n",
    "            for w1 in lst1:\n",
    "                if w1 in keywords1:\n",
    "                    j=j+1\n",
    "            \n",
    "            \n",
    "            f=(i-j+100)/2\n",
    "            #print(\"length of sentence\",len(t.split()))\n",
    "            if(len(t.split())<=8):\n",
    "                f=f-1\n",
    "            lenofsen=len(t)\n",
    "            if(lenofsen>senlength):\n",
    "                senlength=lenofsen\n",
    "                lensen = lensen+1\n",
    "                f=f+lensen\n",
    "            if(num_there(t)==True):\n",
    "                f=f+1\n",
    "            #f=f+count\n",
    "            f=f+shannon(t)\n",
    "            fc.append(f)\n",
    "            \n",
    "        for i in range(len(fc)):\n",
    "            li.append([fc[i],i])\n",
    "        li.sort(reverse=True)\n",
    "        #print(\"this is li\",li)\n",
    "        \n",
    "        sort_index = []\n",
    "        for x in li:\n",
    "            sort_index.append(x[1])\n",
    "        #print(sort_index)\n",
    "        compressionratio = round((len(sort_index)*50)/100)\n",
    "        #print(\"this is compression ratio\",compressionratio)\n",
    "        newsen=[]\n",
    "        for j in range(compressionratio):\n",
    "            newsen.append(sentences[sort_index[j]])\n",
    "        #print(\"this is newsen\",newsen)    \n",
    "        originalfull_str = convert_list_to_string(newsen)\n",
    "        originalfull_str=\" \".join(originalfull_str.split())\n",
    "        rouge_1=comparerouge1(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",originalfull_str,filename)\n",
    "        rouge_2=comparerouge2(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",originalfull_str,filename)\n",
    "        rouge_l=comparerougel(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",originalfull_str,filename)\n",
    "        rouge_w=comparerougew(\"C:/Users/HM-AM/Documents/GitHub/Gujarati-Textsummarization-corpus/gujarati Text summarization dataset/economics/annotator2/\",originalfull_str,filename)\n",
    "        \n",
    "        rouge1.append(rouge_1)\n",
    "        rouge2.append(rouge_2)\n",
    "        rougel.append(rouge_l)\n",
    "        rougew.append(rouge_w)\n",
    "        print(rouge1)\n",
    "        rouge1average=Average(rouge1)\n",
    "        rouge2average=Average(rouge2)\n",
    "        rougelaverage=Average(rougel)\n",
    "        rougewaverage=Average(rougew)\n",
    "        print(\"rouge1 average\",rouge1average)\n",
    "        print(\"rouge2 average\",rouge2average)\n",
    "        print(\"rougel average\",rougelaverage)\n",
    "        print(\"rougew average\",rougewaverage)\n",
    "        \n",
    "        n=str(n)\n",
    "        file1.write(originalfull_str)\n",
    "        fc.clear()\n",
    "        li.clear()\n",
    "#         for i in range(len(fc)):\n",
    "#             li.append([fc[i],i])\n",
    "#         li.sort(reverse=True)\n",
    "#         sort_index = []\n",
    "#         for x in li:\n",
    "#             sort_index.append(x[1])\n",
    "        #print(sort_index)\n",
    "        \n",
    "                \n",
    "        instances = {}\n",
    "    \n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#for root, dirs, files in os.walk(\"/home/harsh/Documents/gujdocs/gsf2003/\"):\n",
    "for root, dirs, files in os.walk(\"G:/Movies/gujarati Text summarization dataset/economics/\"):    \n",
    "#for root, dirs, files in os.walk(\"G:/Movies/post_1/\"):    \n",
    "    for file in files:\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(root, file), 'r') as f:\n",
    "                #print(os.path.join(root, file))\n",
    "                load_file8(os.path.join(root, file),\"G:/Movies/gujarati Text summarization datasetformal/economics/\"+file)\n",
    "                #load_file8(os.path.join(root, file),\"G:/Movies/post_1_summary/\"+file)\n",
    "#average = Average(fc)\n",
    "#print(average)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
